{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC32l2mRqnPv"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0trox4maqp5D",
        "outputId": "1790badf-e7e1-4442-f723-b32254c288e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1hdtwNusU8T",
        "outputId": "70027f02-9b03-4fef-cc87-192512bce2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0 xxhash-3.4.1\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.1\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=e01cbd30329fa9122d819cadac8e5199ac5107df09419c6992578e8ab55d2063\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "!pip install sacrebleu\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UDiYFUQiqnPw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "# from datasets import metric\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "F46l21msqnPx"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH=500\n",
        "BATCH_SIZE=16"
      ],
      "metadata": {
        "id": "sZhreFAZDRXk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We1Dr8PjqnPy"
      },
      "source": [
        "## 2. Load Dataset & Prepocess the Data\n",
        "- Load Train and Test dataset\n",
        "- Truncate the answers to MAX_LENGTH.\n",
        "- Concat Question and Answer using special token, and add start and end token.\n",
        "-  Repeate the above two steps for Train and Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dEQfwlC_qnPy"
      },
      "outputs": [],
      "source": [
        "train_test_dict={\"train\":{},\"test\":{}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qIQkrjyYqnPy"
      },
      "outputs": [],
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained('distilgpt2', add_special_tokens=True,additional_special_tokens=['[response]'], pad_token=\"[Pad]\",padding_side='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pfel76djqnPy"
      },
      "outputs": [],
      "source": [
        "bos_token=tokenizer.decode(tokenizer.bos_token_id)\n",
        "eos_token=tokenizer.decode(tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "M5f-4Lv-qnPy"
      },
      "outputs": [],
      "source": [
        "for key in train_test_dict:\n",
        "    dataset_pd=pd.read_csv(f'/content/drive/MyDrive/NLP_Project/{key}_datasets/MedQuAD_{key}.csv')\n",
        "    dataset_pd=dataset_pd.astype('string')\n",
        "    dataset_pd['Question']=bos_token + dataset_pd['Question'] + ' [response] '\n",
        "    dataset_pd['QA_pairs'] = dataset_pd['Question']  + dataset_pd['Answer_cut'] + eos_token\n",
        "    dataset_list=list(dataset_pd['QA_pairs'])\n",
        "    questions_list=list(dataset_pd['Question'])\n",
        "    train_test_dict[key]['dataset']=dataset_pd\n",
        "    train_test_dict[key]['question_list']=questions_list\n",
        "    train_test_dict[key]['QA_list']=dataset_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThimztpQqnPy"
      },
      "source": [
        "## 3. Initialize pytorch dataset and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HZRdohwvqnPy"
      },
      "outputs": [],
      "source": [
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, data,  tokenizer, questions):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data=self.tokenizer(data, padding=True, truncation=True ,return_tensors='pt')\n",
        "        self.non_tokenized_data= data\n",
        "        self.tokenized_questions=self.tokenizer(questions, padding=True, truncation=False, return_tensors='pt')\n",
        "    def __len__(self):\n",
        "        return len(self.data['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data['input_ids'][idx], self.data['attention_mask'][idx], self.non_tokenized_data[idx],self.tokenized_questions['input_ids'][idx], self.tokenized_questions['attention_mask'][idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MedicalDataset(train_test_dict['train']['QA_list'], tokenizer, train_test_dict['train']['question_list'])\n",
        "test_dataset = MedicalDataset(train_test_dict['test']['QA_list'], tokenizer, train_test_dict['test']['question_list'])\n"
      ],
      "metadata": {
        "id": "htN_v7CJt2cc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HdwHz0j7qnPz"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MojdKzBPqnPz"
      },
      "source": [
        "## 4. Define the model\n",
        "- Resize the total number of model's embeddings  as we added a special token ('['response']') for sperating the question from the answer.\n",
        "- Set the model's hyperparams, optimizer and the learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NMyQ9f1CqnPz"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model=model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UA23xUi8qnPz"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler=get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps= epochs*(len(train_loader)))\n",
        "grad_accumulatio_steps=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9TMRUIIqnPz"
      },
      "source": [
        "## 6. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4zef4ZzqnPz"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_string = 'fine_tune_model_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "train_loss_values=[]\n",
        "test_loss_values=[]\n",
        "ctr=1\n",
        "for epoch in range(epochs): # number of epochs\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    model.train()\n",
        "    for input_ids, attention_mask,raw_data, toknized_questions_id,toknized_questions_mask in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids=input_ids.to(device)\n",
        "        attention_mask=attention_mask.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        train_loss+=loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # if (grad_accumulatio_steps != 0) and  ((grad_accumulatio_steps %grad_accumulatio_steps) ==0):\n",
        "        #     optimizer.step()\n",
        "        #     scheduler.step()\n",
        "        #     ctr+=1\n",
        "    model.eval()\n",
        "    val_loss=0\n",
        "    with torch.no_grad():\n",
        "      for input_ids, attention_mask, raw_data, toknized_questions_id,toknized_questions_mask in tqdm(test_loader):\n",
        "        input_ids=input_ids.to(device)\n",
        "        attention_mask=attention_mask.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        val_loss += outputs.loss.item()\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    test_loss_values.append(avg_val_loss)\n",
        "\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch}, train_loss: {avg_train_loss}, val_loss: {avg_val_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Mi5juvbyqnP0"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/NLP_Project/models/fineTune_models/MedQuad_left_pad_Latest_fineTuned_gpt2_3epoch'+date_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqW4dUZCqnP0"
      },
      "source": [
        "## 7. Model Evaluation\n",
        "- Generating the sacreBleu and Rouge scores on test datasset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6E8BeTCwqnP0"
      },
      "outputs": [],
      "source": [
        "model=GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/NLP_Project/models/fineTune_models/MedQuad_left_pad_Latest_fineTuned_gpt2_3epochfine_tune_model_2024_04_06_14_18_29/').to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "USKJBuGmqnP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adee9ca8-9a2e-42fe-f34d-8e7d783812a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205/205 [10:17<00:00,  3.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bleu: 6.27,  Rouge:0.208, Rouge:0.175\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "bleu=evaluate.load('sacrebleu')\n",
        "rouge=evaluate.load('rouge')\n",
        "\n",
        "for _, _, raw_data, toknized_questions_id,toknized_questions_mask in tqdm(test_loader):\n",
        "    input_ids=toknized_questions_id.to(device)\n",
        "    attention_mask=toknized_questions_mask.to(device)\n",
        "    answers=[qa.split('[response]')[-1] for qa in raw_data]\n",
        "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask,  pad_token_id=tokenizer.pad_token_id,max_new_tokens=MAX_LENGTH)\n",
        "    decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    for pred, ref in zip(answers, decoded_outputs):\n",
        "        bleu.add(references = ref, predictions=pred)\n",
        "        rouge.add(references = ref, predictions=pred)\n",
        "\n",
        "bleu=bleu.compute()\n",
        "rouge=rouge.compute()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Bleu: {bleu['score']},  Rouge:{rouge['rouge1']}, Rouge:{rouge['rougeL']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3) Chekcing the reponse for any one question."
      ],
      "metadata": {
        "id": "ey72AE7I_mqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hNLIKH82qnP1"
      },
      "outputs": [],
      "source": [
        "output_1391=model.generate(input_ids=torch.tensor(tokenizer(questions_list[11])['input_ids']).reshape(1,-1).to(device),attention_mask=torch.tensor(tokenizer(questions_list[11])['attention_mask']).reshape(1,-1).to(device), pad_token_id=tokenizer.pad_token_id,max_new_tokens=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "walV9OZFOq4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer(bos_token+questions_list[11])['input_ids'][1:],skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gE2oOFXAo7w1",
        "outputId": "2bd77eeb-6909-4318-a6dc-ea678d53ab4a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|>What to do for Gastritis? [response] '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output_1391[0], skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "tkXTewd_oNT_",
        "outputId": "c8fabd4f-ff91-499d-a6ab-54f0180b6c5f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|>What to do for Gastritis? [response]  Gastritis is a disease in which the body does not make enough of a protein called a substance called a protein.  The body does not make enough of a protein called a substance called a protein. The body does not make enough of a protein called a substance called a protein. Gastritis is a disease in which the body does not make enough of a protein called a substance called a protein. The body does not make enough of a protein called a substance called a protein. Gastritis is a disease in which the body does not make enough of a protein called a substance. The body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make enough of a protein called a substance. Gastritis is a disease in which the body does not make'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}